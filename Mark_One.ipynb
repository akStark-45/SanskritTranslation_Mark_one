{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "HYblTT2DSYJ_"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TXsQcXErTEQp",
        "outputId": "febb16c3-36ff-4bf3-ec65-443cb86b921a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "folder_path = \"/content/drive/MyDrive/Translation\"\n",
        "os.chdir(folder_path)"
      ],
      "metadata": {
        "id": "BXt8V28AUWBE"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_excel('data.xlsx')\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "gAVwi_TgU58E",
        "outputId": "6a465d39-4c55-4b38-fbd9-c35f48ce695d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Sanskrit Word                                   English Meanings\n",
              "0  Abhanavarana:  ['Screening the outshining Bragman', ' one of ...\n",
              "1        Abhasa:  ['Reflection', ' appearance', ' semblance', ' ...\n",
              "2   Abhasamatra:                                  ['In name only.']\n",
              "3    Abhasavada:  ['Doctrine holding that all creation is reflec...\n",
              "4        Abhati:                          ['Shines', ' illumines.']"
            ],
            "text/html": [
              "\n",
              "\n",
              "  <div id=\"df-8d1a242d-a331-47ec-b777-fdbc6a9c92a6\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sanskrit Word</th>\n",
              "      <th>English Meanings</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Abhanavarana:</td>\n",
              "      <td>['Screening the outshining Bragman', ' one of ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Abhasa:</td>\n",
              "      <td>['Reflection', ' appearance', ' semblance', ' ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Abhasamatra:</td>\n",
              "      <td>['In name only.']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Abhasavada:</td>\n",
              "      <td>['Doctrine holding that all creation is reflec...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Abhati:</td>\n",
              "      <td>['Shines', ' illumines.']</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8d1a242d-a331-47ec-b777-fdbc6a9c92a6')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "\n",
              "\n",
              "\n",
              "    <div id=\"df-4d7a80f5-657c-42d9-a2f9-f72aea24591f\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4d7a80f5-657c-42d9-a2f9-f72aea24591f')\"\n",
              "              title=\"Suggest charts.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "    </div>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "    <script>\n",
              "      async function quickchart(key) {\n",
              "        const containerElement = document.querySelector('#' + key);\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      }\n",
              "    </script>\n",
              "\n",
              "      <script>\n",
              "\n",
              "function displayQuickchartButton(domScope) {\n",
              "  let quickchartButtonEl =\n",
              "    domScope.querySelector('#df-4d7a80f5-657c-42d9-a2f9-f72aea24591f button.colab-df-quickchart');\n",
              "  quickchartButtonEl.style.display =\n",
              "    google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "}\n",
              "\n",
              "        displayQuickchartButton(document);\n",
              "      </script>\n",
              "      <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8d1a242d-a331-47ec-b777-fdbc6a9c92a6 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8d1a242d-a331-47ec-b777-fdbc6a9c92a6');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_YyyYQaVHoG",
        "outputId": "028f4d81-37c2-4d12-f7fc-ec211e9be2cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['Sanskrit Tokenized'] = df['Sanskrit Word'].apply(nltk.word_tokenize)"
      ],
      "metadata": {
        "id": "EOtDdlkDWZS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy"
      ],
      "metadata": {
        "id": "xWZpyz_pXPQ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp_english = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "Mn61vdyTXpF9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_english_sentence(english_sentence):\n",
        "    english_doc = nlp_english(english_sentence)\n",
        "    return [token.text for token in english_doc]\n",
        "\n",
        "df['English Tokenized'] = df['English Meanings'].apply(tokenize_english_sentence)"
      ],
      "metadata": {
        "id": "aqQiMf-8YFJP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec"
      ],
      "metadata": {
        "id": "f036uHgUYIrB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train or load Word2Vec model on the tokenized words for Sanskrit\n",
        "word2vec_model_sanskrit = Word2Vec(sentences=df['Sanskrit Tokenized'], vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Train or load Word2Vec model on the tokenized words for English\n",
        "word2vec_model_english = Word2Vec(sentences=df['English Tokenized'], vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Convert tokenized words to word embeddings for Sanskrit\n",
        "df['Sanskrit Embeddings'] = df['Sanskrit Tokenized'].apply(lambda tokens: [word2vec_model_sanskrit.wv[word] for word in tokens])\n",
        "\n",
        "# Convert tokenized words to word embeddings for English\n",
        "df['English Embeddings'] = df['English Tokenized'].apply(lambda tokens: [word2vec_model_english.wv[word] for word in tokens])"
      ],
      "metadata": {
        "id": "jmeSRBtnYiMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# Create a tokenizer for Sanskrit words\n",
        "sanskrit_tokenizer = Tokenizer(oov_token='<OOV>')\n",
        "sanskrit_tokenizer.fit_on_texts(df['Sanskrit Tokenized'])\n",
        "\n",
        "# Create a tokenizer for English words\n",
        "english_tokenizer = Tokenizer(oov_token='<OOV>')\n",
        "english_tokenizer.fit_on_texts(df['English Tokenized'])"
      ],
      "metadata": {
        "id": "qXLCowKvZo6v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert tokenized Sanskrit sequences to index sequences\n",
        "sanskrit_sequences = sanskrit_tokenizer.texts_to_sequences(df['Sanskrit Tokenized'])\n",
        "\n",
        "# Convert tokenized English sequences to index sequences\n",
        "english_sequences = english_tokenizer.texts_to_sequences(df['English Tokenized'])"
      ],
      "metadata": {
        "id": "PbB_sNYdaqUp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Define the maximum sequence lengths (input and output)\n",
        "max_sanskrit_length = 20\n",
        "max_english_length = 25\n",
        "\n",
        "# Pad or truncate the sequences to the maximum lengths\n",
        "padded_sanskrit_sequences = pad_sequences(sanskrit_sequences, maxlen=max_sanskrit_length, padding='post', truncating='post')\n",
        "padded_english_sequences = pad_sequences(english_sequences, maxlen=max_english_length, padding='post', truncating='post')"
      ],
      "metadata": {
        "id": "sIaUf4T_ayWy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Input will be the padded Sanskrit sequences\n",
        "encoder_input_data = padded_sanskrit_sequences\n",
        "\n",
        "# Output will be the padded English sequences shifted by one time step (teacher forcing)\n",
        "decoder_input_data = padded_english_sequences[:, :-1]\n",
        "decoder_output_data = padded_english_sequences[:, 1:]\n",
        "\n",
        "# The first time step of the output is not used (remove the first token from each sequence)\n",
        "# The last token in each output sequence is <PAD> due to padding, and it won't have a corresponding input token\n",
        "# To maintain consistency, the last token in each output sequence is also removed"
      ],
      "metadata": {
        "id": "6q6skngQa4R_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the input shape for the model\n",
        "input_shape = (max_sanskrit_length,)\n",
        "\n",
        "# Define the size of the vocabulary for both Sanskrit and English\n",
        "sanskrit_vocab_size = len(sanskrit_tokenizer.word_index) + 1\n",
        "english_vocab_size = len(english_tokenizer.word_index) + 1\n",
        "\n",
        "# Define the embedding dimension\n",
        "embedding_dim = 100\n",
        "\n",
        "# Encoder\n",
        "encoder_input = Input(shape=input_shape)\n",
        "encoder_embedding = Embedding(input_dim=sanskrit_vocab_size, output_dim=embedding_dim)(encoder_input)\n",
        "encoder_lstm = LSTM(300)(encoder_embedding)  # Adjust the number of LSTM units as needed\n",
        "\n",
        "# Decoder\n",
        "decoder_input = Input(shape=(max_english_length - 1,))\n",
        "decoder_embedding = Embedding(input_dim=english_vocab_size, output_dim=embedding_dim)(decoder_input)\n",
        "decoder_lstm = LSTM(300, return_sequences=True)(decoder_embedding, initial_state=[encoder_lstm, encoder_lstm])\n",
        "decoder_output = Dense(english_vocab_size, activation='softmax')(decoder_lstm)\n",
        "\n",
        "# Model\n",
        "model = Model(inputs=[encoder_input, decoder_input], outputs=decoder_output)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Print the summary of the model\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S2nmokbTa8YL",
        "outputId": "209d6d1c-bf10-4f51-bdd9-1457c8072fe4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_5 (InputLayer)           [(None, 20)]         0           []                               \n",
            "                                                                                                  \n",
            " input_6 (InputLayer)           [(None, 24)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding_4 (Embedding)        (None, 20, 100)      248600      ['input_5[0][0]']                \n",
            "                                                                                                  \n",
            " embedding_5 (Embedding)        (None, 24, 100)      384900      ['input_6[0][0]']                \n",
            "                                                                                                  \n",
            " lstm_4 (LSTM)                  (None, 300)          481200      ['embedding_4[0][0]']            \n",
            "                                                                                                  \n",
            " lstm_5 (LSTM)                  (None, 24, 300)      481200      ['embedding_5[0][0]',            \n",
            "                                                                  'lstm_4[0][0]',                 \n",
            "                                                                  'lstm_4[0][0]']                 \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 24, 3849)     1158549     ['lstm_5[0][0]']                 \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 2,754,449\n",
            "Trainable params: 2,754,449\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model using provided data\n",
        "model.fit([encoder_input_data, decoder_input_data], decoder_output_data, batch_size=32, epochs=80, validation_split=0.1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hPQzo9LbbZgU",
        "outputId": "63ba53e8-fae8-464a-f806-6ccefe336b98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/80\n",
            "71/71 [==============================] - 14s 147ms/step - loss: 4.0134 - accuracy: 0.4880 - val_loss: 3.3168 - val_accuracy: 0.4931\n",
            "Epoch 2/80\n",
            "71/71 [==============================] - 5s 75ms/step - loss: 2.7936 - accuracy: 0.5511 - val_loss: 2.7840 - val_accuracy: 0.5567\n",
            "Epoch 3/80\n",
            "71/71 [==============================] - 3s 38ms/step - loss: 2.4122 - accuracy: 0.6278 - val_loss: 2.5335 - val_accuracy: 0.6324\n",
            "Epoch 4/80\n",
            "71/71 [==============================] - 1s 14ms/step - loss: 2.1700 - accuracy: 0.6743 - val_loss: 2.4033 - val_accuracy: 0.6508\n",
            "Epoch 5/80\n",
            "71/71 [==============================] - 2s 27ms/step - loss: 2.0437 - accuracy: 0.6902 - val_loss: 2.3637 - val_accuracy: 0.6632\n",
            "Epoch 6/80\n",
            "71/71 [==============================] - 1s 20ms/step - loss: 1.9725 - accuracy: 0.6965 - val_loss: 2.3530 - val_accuracy: 0.6667\n",
            "Epoch 7/80\n",
            "71/71 [==============================] - 2s 23ms/step - loss: 1.9179 - accuracy: 0.7011 - val_loss: 2.3409 - val_accuracy: 0.6682\n",
            "Epoch 8/80\n",
            "71/71 [==============================] - 1s 15ms/step - loss: 1.8706 - accuracy: 0.7043 - val_loss: 2.3428 - val_accuracy: 0.6710\n",
            "Epoch 9/80\n",
            "71/71 [==============================] - 1s 16ms/step - loss: 1.8264 - accuracy: 0.7074 - val_loss: 2.3426 - val_accuracy: 0.6736\n",
            "Epoch 10/80\n",
            "71/71 [==============================] - 1s 20ms/step - loss: 1.7849 - accuracy: 0.7120 - val_loss: 2.3435 - val_accuracy: 0.6739\n",
            "Epoch 11/80\n",
            "71/71 [==============================] - 2s 26ms/step - loss: 1.7456 - accuracy: 0.7160 - val_loss: 2.3471 - val_accuracy: 0.6736\n",
            "Epoch 12/80\n",
            "71/71 [==============================] - 2s 22ms/step - loss: 1.7077 - accuracy: 0.7192 - val_loss: 2.3539 - val_accuracy: 0.6759\n",
            "Epoch 13/80\n",
            "71/71 [==============================] - 1s 18ms/step - loss: 1.6713 - accuracy: 0.7227 - val_loss: 2.3567 - val_accuracy: 0.6761\n",
            "Epoch 14/80\n",
            "71/71 [==============================] - 1s 16ms/step - loss: 1.6366 - accuracy: 0.7253 - val_loss: 2.3684 - val_accuracy: 0.6779\n",
            "Epoch 15/80\n",
            "71/71 [==============================] - 1s 13ms/step - loss: 1.6035 - accuracy: 0.7276 - val_loss: 2.3739 - val_accuracy: 0.6799\n",
            "Epoch 16/80\n",
            "71/71 [==============================] - 1s 13ms/step - loss: 1.5733 - accuracy: 0.7300 - val_loss: 2.3883 - val_accuracy: 0.6787\n",
            "Epoch 17/80\n",
            "71/71 [==============================] - 1s 16ms/step - loss: 1.5444 - accuracy: 0.7323 - val_loss: 2.3970 - val_accuracy: 0.6807\n",
            "Epoch 18/80\n",
            "71/71 [==============================] - 1s 13ms/step - loss: 1.5154 - accuracy: 0.7350 - val_loss: 2.4170 - val_accuracy: 0.6781\n",
            "Epoch 19/80\n",
            "71/71 [==============================] - 1s 15ms/step - loss: 1.4889 - accuracy: 0.7368 - val_loss: 2.4291 - val_accuracy: 0.6792\n",
            "Epoch 20/80\n",
            "71/71 [==============================] - 1s 14ms/step - loss: 1.4625 - accuracy: 0.7392 - val_loss: 2.4384 - val_accuracy: 0.6792\n",
            "Epoch 21/80\n",
            "71/71 [==============================] - 1s 13ms/step - loss: 1.4354 - accuracy: 0.7420 - val_loss: 2.4506 - val_accuracy: 0.6769\n",
            "Epoch 22/80\n",
            "71/71 [==============================] - 1s 21ms/step - loss: 1.4104 - accuracy: 0.7431 - val_loss: 2.4644 - val_accuracy: 0.6784\n",
            "Epoch 23/80\n",
            "71/71 [==============================] - 1s 21ms/step - loss: 1.3872 - accuracy: 0.7457 - val_loss: 2.4778 - val_accuracy: 0.6768\n",
            "Epoch 24/80\n",
            "71/71 [==============================] - 1s 21ms/step - loss: 1.3614 - accuracy: 0.7483 - val_loss: 2.4863 - val_accuracy: 0.6794\n",
            "Epoch 25/80\n",
            "71/71 [==============================] - 1s 13ms/step - loss: 1.3362 - accuracy: 0.7506 - val_loss: 2.5003 - val_accuracy: 0.6789\n",
            "Epoch 26/80\n",
            "71/71 [==============================] - 1s 16ms/step - loss: 1.3128 - accuracy: 0.7534 - val_loss: 2.5129 - val_accuracy: 0.6786\n",
            "Epoch 27/80\n",
            "71/71 [==============================] - 1s 14ms/step - loss: 1.2877 - accuracy: 0.7572 - val_loss: 2.5247 - val_accuracy: 0.6797\n",
            "Epoch 28/80\n",
            "71/71 [==============================] - 1s 16ms/step - loss: 1.2641 - accuracy: 0.7601 - val_loss: 2.5419 - val_accuracy: 0.6791\n",
            "Epoch 29/80\n",
            "71/71 [==============================] - 1s 16ms/step - loss: 1.2398 - accuracy: 0.7634 - val_loss: 2.5529 - val_accuracy: 0.6777\n",
            "Epoch 30/80\n",
            "71/71 [==============================] - 1s 16ms/step - loss: 1.2163 - accuracy: 0.7683 - val_loss: 2.5647 - val_accuracy: 0.6797\n",
            "Epoch 31/80\n",
            "71/71 [==============================] - 1s 13ms/step - loss: 1.1933 - accuracy: 0.7708 - val_loss: 2.5764 - val_accuracy: 0.6756\n",
            "Epoch 32/80\n",
            "71/71 [==============================] - 1s 13ms/step - loss: 1.1697 - accuracy: 0.7757 - val_loss: 2.5930 - val_accuracy: 0.6768\n",
            "Epoch 33/80\n",
            "71/71 [==============================] - 1s 17ms/step - loss: 1.1479 - accuracy: 0.7789 - val_loss: 2.6016 - val_accuracy: 0.6777\n",
            "Epoch 34/80\n",
            "71/71 [==============================] - 1s 20ms/step - loss: 1.1258 - accuracy: 0.7832 - val_loss: 2.6152 - val_accuracy: 0.6777\n",
            "Epoch 35/80\n",
            "71/71 [==============================] - 1s 17ms/step - loss: 1.1032 - accuracy: 0.7867 - val_loss: 2.6326 - val_accuracy: 0.6754\n",
            "Epoch 36/80\n",
            "71/71 [==============================] - 1s 18ms/step - loss: 1.0818 - accuracy: 0.7913 - val_loss: 2.6449 - val_accuracy: 0.6756\n",
            "Epoch 37/80\n",
            "71/71 [==============================] - 1s 16ms/step - loss: 1.0605 - accuracy: 0.7943 - val_loss: 2.6564 - val_accuracy: 0.6756\n",
            "Epoch 38/80\n",
            "71/71 [==============================] - 1s 13ms/step - loss: 1.0395 - accuracy: 0.7981 - val_loss: 2.6731 - val_accuracy: 0.6741\n",
            "Epoch 39/80\n",
            "71/71 [==============================] - 1s 14ms/step - loss: 1.0193 - accuracy: 0.8019 - val_loss: 2.6860 - val_accuracy: 0.6739\n",
            "Epoch 40/80\n",
            "71/71 [==============================] - 1s 14ms/step - loss: 0.9991 - accuracy: 0.8069 - val_loss: 2.6994 - val_accuracy: 0.6725\n",
            "Epoch 41/80\n",
            "71/71 [==============================] - 1s 15ms/step - loss: 0.9805 - accuracy: 0.8102 - val_loss: 2.7079 - val_accuracy: 0.6736\n",
            "Epoch 42/80\n",
            "71/71 [==============================] - 1s 15ms/step - loss: 0.9614 - accuracy: 0.8139 - val_loss: 2.7290 - val_accuracy: 0.6711\n",
            "Epoch 43/80\n",
            "71/71 [==============================] - 1s 13ms/step - loss: 0.9434 - accuracy: 0.8181 - val_loss: 2.7381 - val_accuracy: 0.6744\n",
            "Epoch 44/80\n",
            "71/71 [==============================] - 1s 13ms/step - loss: 0.9247 - accuracy: 0.8215 - val_loss: 2.7572 - val_accuracy: 0.6721\n",
            "Epoch 45/80\n",
            "71/71 [==============================] - 1s 13ms/step - loss: 0.9080 - accuracy: 0.8249 - val_loss: 2.7667 - val_accuracy: 0.6733\n",
            "Epoch 46/80\n",
            "71/71 [==============================] - 1s 13ms/step - loss: 0.8914 - accuracy: 0.8286 - val_loss: 2.7782 - val_accuracy: 0.6728\n",
            "Epoch 47/80\n",
            "71/71 [==============================] - 2s 22ms/step - loss: 0.8744 - accuracy: 0.8322 - val_loss: 2.8002 - val_accuracy: 0.6718\n",
            "Epoch 48/80\n",
            "71/71 [==============================] - 1s 19ms/step - loss: 0.8586 - accuracy: 0.8357 - val_loss: 2.8120 - val_accuracy: 0.6733\n",
            "Epoch 49/80\n",
            "71/71 [==============================] - 1s 16ms/step - loss: 0.8429 - accuracy: 0.8383 - val_loss: 2.8236 - val_accuracy: 0.6721\n",
            "Epoch 50/80\n",
            "71/71 [==============================] - 1s 13ms/step - loss: 0.8285 - accuracy: 0.8413 - val_loss: 2.8373 - val_accuracy: 0.6713\n",
            "Epoch 51/80\n",
            "71/71 [==============================] - 1s 14ms/step - loss: 0.8139 - accuracy: 0.8447 - val_loss: 2.8525 - val_accuracy: 0.6721\n",
            "Epoch 52/80\n",
            "71/71 [==============================] - 1s 15ms/step - loss: 0.7988 - accuracy: 0.8468 - val_loss: 2.8616 - val_accuracy: 0.6731\n",
            "Epoch 53/80\n",
            "71/71 [==============================] - 1s 13ms/step - loss: 0.7848 - accuracy: 0.8501 - val_loss: 2.8821 - val_accuracy: 0.6720\n",
            "Epoch 54/80\n",
            "71/71 [==============================] - 1s 16ms/step - loss: 0.7712 - accuracy: 0.8530 - val_loss: 2.8916 - val_accuracy: 0.6693\n",
            "Epoch 55/80\n",
            "71/71 [==============================] - 1s 13ms/step - loss: 0.7582 - accuracy: 0.8561 - val_loss: 2.9129 - val_accuracy: 0.6690\n",
            "Epoch 56/80\n",
            "71/71 [==============================] - 1s 16ms/step - loss: 0.7454 - accuracy: 0.8587 - val_loss: 2.9188 - val_accuracy: 0.6711\n",
            "Epoch 57/80\n",
            "71/71 [==============================] - 1s 14ms/step - loss: 0.7329 - accuracy: 0.8610 - val_loss: 2.9368 - val_accuracy: 0.6711\n",
            "Epoch 58/80\n",
            "71/71 [==============================] - 1s 14ms/step - loss: 0.7194 - accuracy: 0.8636 - val_loss: 2.9496 - val_accuracy: 0.6687\n",
            "Epoch 59/80\n",
            "71/71 [==============================] - 1s 16ms/step - loss: 0.7080 - accuracy: 0.8660 - val_loss: 2.9636 - val_accuracy: 0.6700\n",
            "Epoch 60/80\n",
            "71/71 [==============================] - 1s 17ms/step - loss: 0.6962 - accuracy: 0.8683 - val_loss: 2.9768 - val_accuracy: 0.6703\n",
            "Epoch 61/80\n",
            "71/71 [==============================] - 1s 18ms/step - loss: 0.6848 - accuracy: 0.8714 - val_loss: 2.9879 - val_accuracy: 0.6682\n",
            "Epoch 62/80\n",
            "71/71 [==============================] - 1s 15ms/step - loss: 0.6730 - accuracy: 0.8740 - val_loss: 3.0110 - val_accuracy: 0.6693\n",
            "Epoch 63/80\n",
            "71/71 [==============================] - 1s 13ms/step - loss: 0.6620 - accuracy: 0.8767 - val_loss: 3.0182 - val_accuracy: 0.6718\n",
            "Epoch 64/80\n",
            "71/71 [==============================] - 1s 16ms/step - loss: 0.6514 - accuracy: 0.8782 - val_loss: 3.0281 - val_accuracy: 0.6685\n",
            "Epoch 65/80\n",
            "71/71 [==============================] - 1s 15ms/step - loss: 0.6415 - accuracy: 0.8812 - val_loss: 3.0352 - val_accuracy: 0.6693\n",
            "Epoch 66/80\n",
            "71/71 [==============================] - 1s 13ms/step - loss: 0.6307 - accuracy: 0.8834 - val_loss: 3.0595 - val_accuracy: 0.6703\n",
            "Epoch 67/80\n",
            "71/71 [==============================] - 1s 13ms/step - loss: 0.6222 - accuracy: 0.8847 - val_loss: 3.0727 - val_accuracy: 0.6705\n",
            "Epoch 68/80\n",
            "71/71 [==============================] - 1s 13ms/step - loss: 0.6130 - accuracy: 0.8869 - val_loss: 3.0851 - val_accuracy: 0.6683\n",
            "Epoch 69/80\n",
            "71/71 [==============================] - 1s 14ms/step - loss: 0.6024 - accuracy: 0.8895 - val_loss: 3.0994 - val_accuracy: 0.6687\n",
            "Epoch 70/80\n",
            "71/71 [==============================] - 1s 15ms/step - loss: 0.5926 - accuracy: 0.8913 - val_loss: 3.1198 - val_accuracy: 0.6677\n",
            "Epoch 71/80\n",
            "71/71 [==============================] - 1s 17ms/step - loss: 0.5839 - accuracy: 0.8937 - val_loss: 3.1235 - val_accuracy: 0.6683\n",
            "Epoch 72/80\n",
            "71/71 [==============================] - 1s 18ms/step - loss: 0.5752 - accuracy: 0.8956 - val_loss: 3.1413 - val_accuracy: 0.6667\n",
            "Epoch 73/80\n",
            "71/71 [==============================] - 1s 17ms/step - loss: 0.5667 - accuracy: 0.8974 - val_loss: 3.1546 - val_accuracy: 0.6682\n",
            "Epoch 74/80\n",
            "71/71 [==============================] - 1s 18ms/step - loss: 0.5599 - accuracy: 0.8985 - val_loss: 3.1645 - val_accuracy: 0.6665\n",
            "Epoch 75/80\n",
            "71/71 [==============================] - 1s 15ms/step - loss: 0.5525 - accuracy: 0.8999 - val_loss: 3.1793 - val_accuracy: 0.6696\n",
            "Epoch 76/80\n",
            "71/71 [==============================] - 1s 14ms/step - loss: 0.5447 - accuracy: 0.9023 - val_loss: 3.1989 - val_accuracy: 0.6660\n",
            "Epoch 77/80\n",
            "71/71 [==============================] - 1s 13ms/step - loss: 0.5362 - accuracy: 0.9041 - val_loss: 3.2129 - val_accuracy: 0.6682\n",
            "Epoch 78/80\n",
            "71/71 [==============================] - 1s 14ms/step - loss: 0.5280 - accuracy: 0.9056 - val_loss: 3.2214 - val_accuracy: 0.6677\n",
            "Epoch 79/80\n",
            "71/71 [==============================] - 1s 15ms/step - loss: 0.5230 - accuracy: 0.9063 - val_loss: 3.2305 - val_accuracy: 0.6670\n",
            "Epoch 80/80\n",
            "71/71 [==============================] - 1s 14ms/step - loss: 0.5160 - accuracy: 0.9078 - val_loss: 3.2487 - val_accuracy: 0.6650\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7a3128175a80>"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# New Sanskrit word for translation\n",
        "new_sanskrit_word = \"Abhimani:\"\n",
        "\n",
        "# Tokenize and pad the new Sanskrit word\n",
        "new_sanskrit_sequences = sanskrit_tokenizer.texts_to_sequences([new_sanskrit_word])\n",
        "padded_new_sanskrit_sequences = pad_sequences(new_sanskrit_sequences, maxlen=max_sanskrit_length, padding='post', truncating='post')\n",
        "\n",
        "# Perform inference for translation\n",
        "encoder_input_for_translation = padded_new_sanskrit_sequences\n",
        "decoder_input_for_translation = np.zeros((1, max_english_length - 1))  # Initialize decoder input with zeros\n",
        "start_token_index = 1  # Default to index 1 as the start token index\n",
        "if '<start>' in english_tokenizer.word_index:\n",
        "    start_token_index = english_tokenizer.word_index['<start>']\n",
        "decoder_input_for_translation[:, 0] = start_token_index  # Set the start token index\n",
        "translated_word_sequence = []  # To store the translated word sequence\n",
        "\n",
        "max_translation_length = 20  # Set a maximum translation length (adjust as needed)\n",
        "\n",
        "for i in range(max_translation_length - 1):  # Loop until the maximum translation length\n",
        "    predictions = model.predict([encoder_input_for_translation, decoder_input_for_translation])\n",
        "    next_word_index = np.argmax(predictions[:, i, :], axis=-1)  # Get the index of the next word in the sequence\n",
        "    translated_word_sequence.append(int(next_word_index))  # Convert numpy int to Python int\n",
        "    decoder_input_for_translation[:, i+1] = next_word_index  # Update decoder input for the next time step\n",
        "\n",
        "    if next_word_index == 0:  # Check if the predicted word is the padding token (index 0)\n",
        "        break\n",
        "\n",
        "# Convert the translated_word_sequence back to English words\n",
        "translated_english_words = english_tokenizer.sequences_to_texts([translated_word_sequence])[0]\n",
        "\n",
        "print(\"Sanskrit Word:\", new_sanskrit_word)\n",
        "print(\"Translated English Meaning:\", translated_english_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iUkPnsEHhGL0",
        "outputId": "a93ea1f5-b9f4-419f-b8cd-f3e3f458a6dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 658ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "Sanskrit Word: Abhimani:\n",
            "Translated English Meaning: ' the state of being an experiencer or enjoyer . ' ] <OOV>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1VVj_f5EIbtM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}